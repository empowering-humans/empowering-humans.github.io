<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="static/css/index.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <script defer src="./static/js/fontawesome.all.min.js"></script>

    <title>Learning to Assist Humans without Inferring Rewards</title>

    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
        },
        svg: {
          fontCache: "global",
        },
      }
    </script>
    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>
  </head>
  <body>
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          macros: {
            human: "\\textbf{H}",
            robot: "\\textbf{R}",
            ar: "a^{\\human}",
            ah: "a^{\\robot}",
          },
        },
        svg: {
          fontCache: "global",
        },
      }
    </script>
    <header>
      <h1>Learning to Assist Humans without Inferring Rewards</h1>
      <div class="authors">
        <span class="author affil-1">Vivek Myers</span> <span class="author affil-1">Evan Ellis</span>
        <span class="author affil-2">Ben Eysenbach</span> <span class="author affil-1">Sergey Levine</span>
        <span class="author affil-1">Anca Dragan</span>
      </div>
      <div class="affiliations">
        <span class="university affil-1">UC Berkeley</span>
        <span class="university affil-2">Princeton University</span>
      </div>
      <div class="links">
        <span class="link">
          <a href="./static/pdf/paper.pdf" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
          </a>
        </span>
        <span class="link">
          <a href="about:blank" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
          </a>
        </span>
        <span class="link">
          <a href="about:blank" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="fas fa-video"></i></span><span>Video</span>
          </a>
        </span>
        <span class="link">
          <a href="about:blank" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
          </a>
        </span>
      </div>
    </header>

    <main>
      <div class="centered">
        <img style="margin: 0" src="figures/coord_ring_collab_onion-cut.gif" />
      </div>
      <div class="centered">
        <div class="gif_legend">
          <img src="figures/human_triangle.svg" />
          <p>Human Policy</p>
        </div>
        <div class="gif_legend">
          <img src="figures/esr_triangle.svg" />
          <p>Empowerment Policy (ESR)</p>
        </div>
      </div>
      <section>
        <h2>Abstract</h2>
        <p>
          Assistive agents should make humans' lives easier. Classically, such assistance is studied through the lens of
          inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human's intention
          and then selects actions to help the human reach that goal. This approach requires inferring intentions, which
          can be difficult in high-dimensional settings. We build upon prior work that studies assistance through the
          lens of empowerment: an assistive agent aims to maximize the influence of the human's actions such that they
          exert a greater control over the environmental outcomes and can solve tasks in fewer steps. We lift the major
          limitation of prior work in this area---scalability to high-dimensional settings---with contrastive successor
          representations. We formally prove that these representations estimate a similar notion of empowerment to that
          studied by prior work and provide a ready-made mechanism for optimizing it. Empirically, our proposed method
          outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooperative game setting.
          Theoretically, our work connects ideas from information theory, neuroscience, and reinforcement learning, and
          charts a path for representations to play a critical role in solving assistive problems.
        </p>
      </section>
      <section>
        <h2>Our Method: Empowerment via Successor Representations</h2>
        <p>
          Our core contribution is a novel objective for training agents that are intrinsically motivated to assist
          humans without requiring a model of the human's reward function. Our objective, Empowerment via Successor
          Representations (ESR), maximizes the influence of the human's actions on the environment, and, unlike past
          approaches for assistance without reward inference, is based on a scalable model-free objective that can be
          derived from learned successor features that encode which states the human is likely to want to reach given
          their current action. Our objective empowers the human to reach the desired states, not all states, without
          assuming a human model.
        </p>
        <p>Our method will learn three representations:</p>
        <ol>
          <li>
            $\phi(s,\ar,\ah)$ -- This representation can be understood as a sort of latent-space model, predicting the
            future representation given the current state $s$ and the human's current action $\ah$ as well as the
            robot's current action $\ar$.
          </li>
          <li>
            $\phi'(s,\ar)$ -- This representation can be understood as an uncontrolled model, predicting the
            representation of a future state without reference to the current human action $\ah$. This representation is
            analogous to a value function.
          </li>
          <li>$\psi(s^+)$ -- This is a representation of a future state.</li>
        </ol>
        <p>We then use these learned representations to compute the approximate empowerment reward:</p>
        $$r(s, \ar) = (\phi(s_t, \ar, \ah) - \phi(s_t, \ar))^T \psi(g)$$
        <p>which we use to train an assistive policy <em>without estimating the human's reward.</em></p>
      </section>
      <section>
        <h2>Analyzing the Empowerment Objective</h2>
        <p></p>
      </section>

      <section>
        <h2>Results</h2>
        <p>Present your results here...</p>
        <div class="chart">
          <!-- Replace this div with your actual chart or visualization -->
          <p>Your chart or visualization goes here</p>
        </div>
      </section>

      <section>
        <h2>Discussion</h2>
        <p>Discuss your findings here...</p>
      </section>

      <section>
        <h2>Conclusion</h2>
        <p>Summarize your conclusions here...</p>
      </section>
    </main>

    <script>
      // You can add any necessary JavaScript here
      document.addEventListener("DOMContentLoaded", (event) => {
        console.log("DOM fully loaded and parsed")
        // Add your custom JavaScript functionality here
      })
    </script>
  </body>
</html>
