<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
    <link rel="stylesheet" href="static/css/index.css" />

    <title>Learning to Assist Humans without Inferring Rewards</title>

    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
        },
        svg: {
          fontCache: "global",
        },
      }
    </script>
    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>
    <script>
      function selectContent(query) {
        var range = document.createRange()
        var selection = window.getSelection()
        var elem = document.querySelector(query)
        range.selectNodeContents(elem)
        selection.removeAllRanges()
        selection.addRange(range)
      }
    </script>
  </head>
  <body>
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
          macros: {
            human: "\\textbf{H}",
            robot: "\\textbf{R}",
            ar: "a^{\\robot}",
            ah: "a^{\\human}",
            pih: "\\pi_{\\mathbf{H}}",
            pir: "\\pi_{\\mathbf{R}}",
            E: "\\mathbb{E}",
            S: "\\mathcal{S}",
            sfut: "\\mathfrak{s}^{\\gamma}_{+}",
            emp: "\\mathcal{E}",
          },
        },
        svg: {
          fontCache: "global",
        },
      }
    </script>
    <header>
      <h1>Learning to Assist Humans without Inferring Rewards</h1>
      <div class="authors">
        <span class="author">Vivek Myers</span>
        <span class="author">Evan Ellis</span>
        <span class="affil">1</span>
        <span class="author">Sergey Levine</span>
        <span class="affil">1</span>
        <span class="author">Ben Eysenbach</span>
        <span class="affil">2</span>
        <span class="author">Anca Dragan</span>
        <span class="affil">1</span>
      </div>
      <div class="notes">
        <span class="affil">1</span>
        <span class="institution">UC Berkeley</span>
        <span class="affil">2</span>
        <span class="institution">Princeton University</span>
      </div>
      <div class="links">
        <span class="link">
          <a href="./static/pdf/paper.pdf" target="_blank" class="button">
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
          </a>
        </span>
        <span class="link">
          <a href="https://arxiv.org/abs/2411.02623" class="button">
            <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
          </a>
        </span>
        <!-- <span class="link"> -->
        <!--   <a href="about:blank" class="button"> -->
        <!--     <span class="icon"><i class="fas fa-video"></i></span><span>Video</span> -->
        <!--   </a> -->
        <!-- </span> -->
        <span class="link">
          <a
            href="https://github.com/vivekmyers/empowerment_successor_representations"
            class="button">
            <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
          </a>
        </span>
      </div>
    </header>

    <main>
      <!--      <div class="centered">-->
      <!--        <img style="margin: 0" src="figures/coord_ring_collab_onion-cut.gif" />-->
      <!--      </div>-->
      <!--      <div class="centered">-->
      <!--        <div class="gif_legend">-->
      <!--          <img src="figures/human_triangle.svg" />-->
      <!--          <p>Human Policy</p>-->
      <!--        </div>-->
      <!--        <div class="gif_legend">-->
      <!--          <img src="figures/esr_triangle.svg" />-->
      <!--          <p>Empowerment Policy (ESR)</p>-->
      <!--        </div>-->
      <!--      </div>-->

      <div class="figure margin">
        <img src="figures/coord_ring_demo.svg" />
      </div>
      <section class="abstract">
        <h3>Abstract</h3>
        <p>
          Assistive agents should make humans' lives easier. Classically, such assistance is studied
          through the lens of inverse reinforcement learning, where an assistive agent (e.g., a
          chatbot, a robot) infers a human's intention and then selects actions to help the human
          reach that goal. This approach requires inferring intentions, which can be difficult in
          high-dimensional settings. We build upon prior work that studies assistance through the
          lens of empowerment: an assistive agent aims to maximize the influence of the human's
          actions such that they exert a greater control over the environmental outcomes and can
          solve tasks in fewer steps. We lift the major limitation of prior work in this
          area---scalability to high-dimensional settings---with contrastive successor
          representations. We formally prove that these representations estimate a similar notion of
          empowerment to that studied by prior work and provide a ready-made mechanism for
          optimizing it. Empirically, our proposed method outperforms prior methods on synthetic
          benchmarks, and scales to Overcooked, a cooperative game setting. Theoretically, our work
          connects ideas from information theory, neuroscience, and reinforcement learning, and
          charts a path for representations to play a critical role in solving assistive problems.
        </p>
      </section>
      <section>
        <h2>Our Method: Empowerment via Successor Representations</h2>
        <p>
          Our core contribution is a novel objective for training agents that are intrinsically
          motivated to assist humans without requiring a model of the human's reward function. Our
          objective,
          <b>Empowerment via Successor Representations (ESR)</b>, maximizes the influence of the
          human's actions on the environment, and, unlike past approaches for assistance without
          reward inference, is based on a scalable model-free objective that can be derived from
          learned successor features encoding the states the human may want to reach given their
          action. Our <i>effective empowerment</i> objective enables the human to reach desired
          states without needing to infer their reward function.
        </p>
        <p>Our method learns three representations:</p>
        <ol>
          <li>
            $\phi(s,\ar,\ah)$ – This representation can be understood as a sort of latent-space
            model, predicting the future representation given the current state $s$ and the human's
            current action $\ah$ as well as the robot's current action $\ar$.
          </li>
          <li>
            $\phi'(s,\ar)$ – This representation can be understood as an uncontrolled model,
            predicting the representation of a future state without reference to the current human
            action $\ah$. This representation is analogous to a value function.
          </li>
          <li>$\psi(s^+)$ – This is a representation of a future state.</li>
        </ol>
        <p>
          We then use these learned representations to compute the approximate empowerment reward:
        </p>
        $$r(s, \ar) = \bigl(\phi(s_t, \ar, \ah) - \phi(s_t, \ar)\bigr)^T \psi(g)$$
        <p>
          which we use to train an assistive policy <em>without estimating the human's reward.</em>
        </p>
      </section>
      <section>
        <h2>Analyzing the Effective Empowerment Objective</h2>
        <p>
          The key insight behind the <b>ESR</b> algorithm is that by maximizing a notion of human
          empowerment in the environment, the <i>effective empowerment</i>, we avoid needing to
          infer the human's objectives (or know them in advance).
        </p>
        <figure>
          <div class="margin">
            <div class="triple">
              <img src="figures/simplex.png" />
              <img src="figures/visualizing_skills.png" />
              <img src="figures/visualizing_empowerment_v2.png" />
            </div>
            <div class="triple">
              <p>(a) State marginal polytope</p>
              <p>(b) Mutual information</p>
              <p>(c) Maximizing effective empowerment</p>
            </div>
            <figcaption>
              <b>The Information Geometry of Empowerment.</b> <em>(a)</em> For a given state $s_t$
              and assistant policy $\pir$, we plot the distribution over future states for 6 choices
              of the human policy $\pih$. In a 3-state MDP, we can represent each policy as a vector
              lying on the 2-dimensional probability simplex. We refer to the set of all possible
              state distributions as the <em>state marginal polytope</em>. <em>(b)</em> Mutual
              information corresponds to the distance between the center of the polytope and the
              vertices that are maximally far away. <em>(c)</em> Empowerment corresponds to
              maximizing the size of this polytope. For example, when an assistive agent moves an
              obstacle out of a human user's way, the human user can spend more time at desired
              state.
            </figcaption>
          </div>
        </figure>
        <p>
          In the special case where the human is maximizing a family of reward functions for
          different skills, we show that empowerment enables optimal assistance.
        </p>
        <div class="theorem">
          <span>Assumption 3.1</span>
          <span>Skill Coverage</span>
          The rewards $R \sim \mathcal{R}$ are uniformly distributed over the scaled $|\S|$-simplex
          $\Delta^{|\S|}$ such that: $$\bigl(R+\tfrac{1}{|\S|}\bigr)\bigl(\tfrac{1}{ 1-\gamma
          }\bigr) \sim \operatorname{Unif}\bigl( \Delta^{|\S|} \bigr) =
          \operatorname{Dirichlet}(\hspace{.5ex}\underbrace{\hspace{-.5ex}1,1,\ldots,1\hspace{-.5ex}}_{\text{$|\S|$
          times}}\hspace{.5ex}). \label{eq:uniform}$$
        </div>
        <div class="theorem">
          <span>Assumption 3.2</span> <span>Ergodicity</span>
          For some \(\pih, \pir\), we have $$ \operatorname{P}^{\pih,\pir}(\sfut = s \mid s_0) > 0
          \quad \text{for all } s \in \S, \gamma \in (0,1). \label{eq:ergodic} $$
        </div>
        Our main theoretical result is <b>Theorem 3.1</b>, which shows that under these assumptions,
        maximizing effective empowerment yields a lower bound on the (squared) average-case reward
        achieved by the human for sufficiently large $\gamma$.
        <div class="theorem">
          <span>Theorem 3.1</span>
          Under <b>Assumption 3.1</b> and <b>Assumption 3.2</b> for sufficiently large \(\gamma\)
          and any \(\beta>0\), $$ \emp_{\gamma}(\pih,\pir)^{1/2} \le (\beta /
          e)\,\mathcal{J}_{\pi_{\mathbf{R}}}^\gamma(\pih). $$
        </div>
        The proof is in <b>Appendix B.1</b> of the paper. To the best of our knowledge, this result
        provides the first formal link between empowerment maximization and reward maximization.
        This motivates us to develop a scalable algorithm for empowerment maximization, which we
        introduce in the following section.
      </section>

      <section>
        <h2>Visualizing Training and the Learned Representations</h2>
        <figure>
          <img src="figures/emp_grid_horiz.gif" />
          <figcaption>
            <b>Visualizing Training Empowerment in a 5x5 Gridworld with 10 obstacles.</b> The human
            gets reward from reaching a goal square. The assistive agent can move the obstacles to
            open up a path to the goal. Our method learns to maximize the mutual information between
            the human's action and their future state conditioned on their current state. This
            corresponds to maximizing the volume of the state marginal polytope, which is
            proportional to the number of states that the human can reach from their current
            position. To visualize the representations, we set the latent dimension to 3 instead of
            100.
          </figcaption>
        </figure>
      </section>

      <section>
        <h2>Results</h2>
        <p>
          Our <b>ESR</b> approach learns to assist a simulated human in a Gridworld environment
          augmented with varying numbers of obstacles. As the complexity of the environment
          increases, <b>ESR</b> remains effective, while the other assistance algorithms struggle.
        </p>
        <div class="figure margin">
          <img src="figures/gridplots.svg" class="small plots" />
        </div>
        <p>
          We also evaluate our method in the Overcooked environment, a cooperative cooking game. We
          find that
          <b>ESR</b> outperforms prior methods that assist the human without knowing their reward
          function.
        </p>
        <div class="figure margin">
          <img src="figures/overcooked_table.svg" class="small table" />
        </div>
      </section>

      <section>
        <h2>
          ${\bf B\kern-.05em{\small I\kern-.025em B}\kern-.08em
          T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}$
        </h2>
        <pre
          class="language-bibtex bibtex"
          id="bibtex"
          onclick="selectContent('#bibtex')"><code>@inproceedings{myers2024learninga,
  author    = {Myers, Vivek and Ellis, Evan and Levine, Sergey and Eysenbach, Benjamin and Dragan, Anca},
  booktitle = {{Conference} on {Neural Information Processing Systems}},
  month     = dec,
  title     = {{Learning} to {Assist Humans} without {Inferring Rewards}},
  year      = {2024},
}</code></pre>
      </section>
    </main>
  </body>
</html>
